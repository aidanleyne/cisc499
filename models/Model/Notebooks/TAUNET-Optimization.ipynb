{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a54ae50",
   "metadata": {},
   "source": [
    "# Grid search code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c22fb945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/opt/conda'\n",
    "\n",
    "def shuffle_batch(features, labels):\n",
    "    # Calculate batch size\n",
    "    batch_size = tf.shape(features)[0]\n",
    "    # Create an index to shuffle features and labels in the same order\n",
    "    shuffled_indices = tf.random.shuffle(tf.range(start=0, limit=batch_size))\n",
    "    # Apply gathered indices to shuffle the batch\n",
    "    shuffled_features = tf.gather(features, shuffled_indices)\n",
    "    shuffled_labels = tf.gather(labels, shuffled_indices)\n",
    "    return shuffled_features, shuffled_labels\n",
    "\n",
    "\n",
    "def read_text_files(directory):\n",
    "    labels = []\n",
    "    features = []\n",
    "    # List all files in the directory and sort them\n",
    "    file_paths = sorted([os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.txt')])\n",
    "    # Process each file\n",
    "    for file_path in tqdm(file_paths, total=len(file_paths), desc=\"Processing text files\"):\n",
    "        # Extract user ID (label) from the filename\n",
    "        user_id = int(os.path.basename(file_path).split('_')[0])\n",
    "        # Read the contents of the file, skipping the first line\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()[1:]  # Skip the \"TIME_DELTA\" line\n",
    "            keystrokes = [int(line.strip()) for line in lines]  # Convert to integers\n",
    "        # Append the extracted data to the lists\n",
    "        labels.append(user_id)\n",
    "        features.append(keystrokes)\n",
    "    return labels, features\n",
    "\n",
    "def calculate_rank_n_accuracy(embeddings1, embeddings2, labels1, labels2, n):\n",
    "    correct_matches = 0\n",
    "    for i in range(len(embeddings1)):\n",
    "        # Compute Euclidean distances from embeddings1[i] to all embeddings2\n",
    "        distances = np.linalg.norm(embeddings2 - embeddings1[i], axis=1)\n",
    "        \n",
    "        # Get the indices of the top 10 closest embeddings in embeddings2\n",
    "        closest_indices = np.argsort(distances)[:n]\n",
    "        \n",
    "        # Check if the correct label is within these top 10 closest embeddings\n",
    "        if labels1[i] in labels2[closest_indices]:\n",
    "            correct_matches += 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_matches / len(embeddings1)\n",
    "    return accuracy\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(256, input_shape=(None, 1)),\n",
    "        tf.keras.layers.Dense(128, activation=None),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_dataset(batch_size, shuffle_set):\n",
    "    # Example usage\n",
    "    train_labels, train_features = read_text_files('keystrokes-training')\n",
    "    test_labels, test_features = read_text_files('keystrokes-training')\n",
    "    train_intervals_tensor = tf.constant(train_features)\n",
    "    train_labels_tensor = tf.constant(train_labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_intervals_tensor, train_labels_tensor))\n",
    "    if (shuffle_set == True): \n",
    "        dataset = dataset.map(shuffle_batch)\n",
    "    elif (shuffle_set == False):\n",
    "        pass\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    #return dataset, test_labels, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5fc544d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text files: 100%|██████████| 233840/233840 [02:46<00:00, 1408.43it/s]\n",
      "Processing text files: 100%|██████████| 233840/233840 [00:41<00:00, 5665.32it/s]\n"
     ]
    }
   ],
   "source": [
    "create_dataset(256, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b156c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(test_features, test_labels, optimizer,learning_rate):\n",
    "    intervals_tensor_eval = tf.constant(test_features).numpy()\n",
    "    labels_tensor_eval = tf.constant(test_labels).numpy()\n",
    "    labels1 = labels_tensor_eval[::2]  \n",
    "    labels2 = labels_tensor_eval[1::2]\n",
    "    input_sequences1 = intervals_tensor_eval[::2]  \n",
    "    input_sequences2 = intervals_tensor_eval[1::2]\n",
    "\n",
    "    # Recreate the model architecture\n",
    "    recreated_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(256, input_shape=(None, 1)),\n",
    "        #tf.keras.layers.Dropout(.2),\n",
    "        tf.keras.layers.Dense(128, activation=None),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n",
    "    ])\n",
    "    opt = optimizer(learning_rate=learning_rate)\n",
    "    # Compile the recreated model\n",
    "    recreated_model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tfa.losses.TripletSemiHardLoss()\n",
    "    )\n",
    "                    # Load the weights\n",
    "    recreated_model.load_weights('./saved_model/taunet_model_test.h5')\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings1 = recreated_model.predict(input_sequences1)\n",
    "    embeddings2 = recreated_model.predict(input_sequences2)\n",
    "    accuracies = calculate_rank_n_accuracy(embeddings1, embeddings2, labels1, labels2, 1)\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be0d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text files: 100%|██████████| 25000/25000 [00:03<00:00, 6482.29it/s]\n",
      "Processing text files: 100%|██████████| 2500/2500 [00:00<00:00, 6583.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 20s 24ms/step - loss: nan\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: nan\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: nan\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 18s 24ms/step - loss: nan\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: nan\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: nan\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: nan\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: nan\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: nan\n",
      "Epoch 10/50\n",
      "191/782 [======>.......................] - ETA: 14s - loss: nan"
     ]
    }
   ],
   "source": [
    "def grid_search(param_grid, model_checkpoint_callback):\n",
    "    best_accuracy = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    for learning_rate in param_grid['learning_rate']:\n",
    "        for shuffle_set in param_grid['shuffle']:\n",
    "            for batch_size in param_grid['batch_size']:\n",
    "                for optimizer in param_grid['optimizer']:\n",
    "                    #print(f\"Trying batch size {batch_size}, learning rate {learning_rate}, optimizer {optimizer}\")\n",
    "                    dataset, test_labels, test_features = create_dataset(batch_size, shuffle_set)\n",
    "                    model = create_model()\n",
    "                    opt = optimizer(learning_rate=learning_rate)\n",
    "                    model.compile(optimizer=opt, loss=tfa.losses.TripletSemiHardLoss())\n",
    "                    history = model.fit(dataset, epochs=50, verbose=True, callbacks=[model_checkpoint_callback])  # Adjust epochs as needed\n",
    "                    # Define your parameter grid\n",
    "                    val_accuracy = (evaluate(test_features, test_labels, optimizer, learning_rate))\n",
    "                    if val_accuracy > best_accuracy:\n",
    "                        best_accuracy = val_accuracy\n",
    "                        best_params = {'shuffle': shuffle_set, 'batch_size': batch_size, 'learning_rate': learning_rate, 'optimizer': optimizer}\n",
    "            # Writing best parameters and accuracy to a text file\n",
    "    with open('./best_params_and_accuracy.txt', 'w') as f:\n",
    "        f.write(f\"Best Parameters: {best_params}\\n\")\n",
    "        f.write(f\"Best Accuracy: {best_accuracy}\\n\")\n",
    "        \n",
    "    return best_params, best_accuracy\n",
    "param_grid = {\n",
    "    'shuffle' : [True, False],\n",
    "    'batch_size': [32, 64, 128, 256, 512],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "    'optimizer': [tf.keras.optimizers.Adam, tf.keras.optimizers.SGD]\n",
    "}\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='./saved_model/taunet_model_test.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',  # Change this to 'loss'\n",
    "    mode='min',\n",
    "    save_best_only=False,\n",
    "    verbose=0)\n",
    "\n",
    "grid_search(param_grid, model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b5fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualENV",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
