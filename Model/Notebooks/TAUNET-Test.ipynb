{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d664ae7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1414124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/opt/conda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9a43c",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b13cd",
   "metadata": {},
   "source": [
    "#### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab825ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read in the training textfiles \n",
    "def read_text_files(directory):\n",
    "    labels = []\n",
    "    features = []\n",
    "    # List all files in the directory and sort them\n",
    "    file_paths = sorted([os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.txt')])\n",
    "    # Process each file\n",
    "    for file_path in tqdm(file_paths, total=len(file_paths), desc=\"Processing text files\"):\n",
    "        # Extract user ID (label) from the filename\n",
    "        user_id = int(os.path.basename(file_path).split('_')[0])\n",
    "        # Read the contents of the file, skipping the first line\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()[1:]  # Skip the \"TIME_DELTA\" line\n",
    "            keystrokes = [int(line.strip()) for line in lines]  # Convert to integers\n",
    "        # Append the extracted data to the lists\n",
    "        labels.append(user_id)\n",
    "        features.append(keystrokes)\n",
    "    return labels, features\n",
    "\n",
    "# Function to create instance of Taunet\n",
    "def create_model():\n",
    "    # Define the Taunet model architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(256, input_shape=(None, 1)),  \n",
    "        tf.keras.layers.Dense(128, activation=None),  \n",
    "        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Function to calculate rank n accuracies\n",
    "def calculate_rank_n_accuracy(embeddings1, embeddings2, labels1, labels2, n):\n",
    "    correct_matches = 0\n",
    "    for i in range(len(embeddings1)):\n",
    "        # Compute Euclidean distances from embeddings1[i] to all embeddings2\n",
    "        distances = np.linalg.norm(embeddings2 - embeddings1[i], axis=1)\n",
    "        # Get the indices of the top 10 closest embeddings in embeddings2\n",
    "        closest_indices = np.argsort(distances)[:n]\n",
    "        # Check if the correct label is within these top 10 closest embeddings\n",
    "        if labels1[i] in labels2[closest_indices]:\n",
    "            correct_matches += 1\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_matches / len(embeddings1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d59322",
   "metadata": {},
   "source": [
    "#### Loading Taunet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the model architecture\n",
    "recreated_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(256, input_shape=(None, 1)),\n",
    "    tf.keras.layers.Dense(128, activation=None),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n",
    "])\n",
    "# Compile the recreated model\n",
    "recreated_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tfa.losses.TripletSemiHardLoss()\n",
    ")\n",
    "# Load the weights\n",
    "recreated_model.load_weights('taunet_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3ce8f",
   "metadata": {},
   "source": [
    "#### Building Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory name\n",
    "directory = 'keystrokes-testing'\n",
    "# Read in testing data\n",
    "labels, features = read_text_files(directory)\n",
    "# Convert inter-event differences and parsed user id's into tensorflow object\n",
    "intervals_tensor_eval = tf.constant(features).numpy()\n",
    "labels_tensor_eval = tf.constant(labels).numpy()\n",
    "# Reading every odd and even featyre/label to create dataset with same users but different inter-event sequences\n",
    "labels1 = labels_tensor_eval[::2]  \n",
    "labels2 = labels_tensor_eval[1::2]\n",
    "input_sequences1 = intervals_tensor_eval[::2]  \n",
    "input_sequences2 = intervals_tensor_eval[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23108513",
   "metadata": {},
   "source": [
    "#### Embedding Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa889b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "embeddings1 = recreated_model.predict(input_sequences1)\n",
    "embeddings2 = recreated_model.predict(input_sequences2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5040585e",
   "metadata": {},
   "source": [
    "#### Calculate Rank N Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rank_n_accuracy(embeddings1, embeddings2, labels1, labels2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885af04b",
   "metadata": {},
   "source": [
    "#### Save Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('embeddings1_taunet.csv', embeddings1, delimiter=',', header=','.join(['column{}'.format(i) for i in range(1, embeddings1.shape[1] + 1)]), comments='')\n",
    "np.savetxt('embeddings2_taunet.csv', embeddings2, delimiter=',', header=','.join(['column{}'.format(i) for i in range(1, embeddings2.shape[1] + 1)]), comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualENV",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
